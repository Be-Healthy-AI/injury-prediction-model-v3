#!/usr/bin/env python3
"""Summarize retrospective prediction results and generate enriched visuals."""
from __future__ import annotations

import argparse
import json
from pathlib import Path
from typing import Dict, List, Tuple

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns

import sys

ROOT_DIR = Path(__file__).resolve().parents[2]
if str(ROOT_DIR) not in sys.path:
    sys.path.append(str(ROOT_DIR))

from scripts.backtests.config_utils import BacktestEntry, load_backtest_config  # noqa: E402
from scripts.backtests.insight_utils import (  # noqa: E402
    RISK_CLASS_COLORS,
    RISK_CLASS_LABELS,
    classify_risk,
    compute_risk_series,
    compute_trend_metrics,
    format_severity_probabilities,
    load_pipeline,
    rank_body_part_probabilities,
)

DEFAULT_WINDOWS: Dict[int, Tuple[str, str]] = {
    452607: ("2025-01-01", "2025-02-08"),
    699592: ("2025-01-01", "2025-02-08"),
    258027: ("2025-09-01", "2025-10-29"),
    8198: ("2025-04-01", "2025-05-11"),
    200512: ("2024-04-01", "2024-05-31"),
}

sns.set_theme(style="whitegrid")


def format_prob(value: float | None) -> str:
    if value is None or pd.isna(value):
        return "N/A"
    return f"{value:.3f}"


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description=__doc__)
    parser.add_argument(
        "--config",
        type=Path,
        default=None,
        help="Optional JSON backtesting configuration generated by build_window_config.py.",
    )
    parser.add_argument(
        "--windows",
        type=str,
        default=None,
        help=(
            "Optional custom windows in the format "
            "player_id:start:end,player_id:start:end ... "
            "(dates in YYYY-MM-DD)."
        ),
    )
    parser.add_argument(
        "--predictions-dir",
        type=Path,
        default=Path("backtests") / "predictions",
        help="Directory containing per-model prediction CSVs.",
    )
    parser.add_argument(
        "--daily-features-dir",
        type=Path,
        default=Path("backtests") / "daily_features",
        help="Directory containing trimmed daily feature CSVs per entry.",
    )
    parser.add_argument(
        "--output-dir",
        type=Path,
        default=Path("backtests") / "visualizations",
        help="Directory to store combined outputs and charts.",
    )
    return parser.parse_args()


def parse_windows(raw: str | None) -> Dict[int, Tuple[str, str]]:
    if not raw:
        return DEFAULT_WINDOWS

    windows: Dict[int, Tuple[str, str]] = {}
    for item in raw.split(","):
        parts = item.strip().split(":")
        if len(parts) != 3:
            raise ValueError(f"Invalid window specification: {item!r}")
        player_id, start, end = parts
        windows[int(player_id)] = (start, end)
    return windows


def load_predictions_for_entry(predictions_dir: Path, entry: BacktestEntry) -> pd.DataFrame:
    # Load ensemble predictions (primary)
    ensemble_file = predictions_dir / "ensemble" / entry.predictions_filename("ensemble")
    
    if ensemble_file.exists():
        # Use ensemble predictions
        df = pd.read_csv(ensemble_file)
        df["reference_date"] = pd.to_datetime(df["reference_date"])
        df = df.sort_values("reference_date")
        df["ensemble"] = df["injury_probability"]
        pivot = df[["player_id", "player_name", "reference_date", "ensemble"]].copy()
        pivot["entry_id"] = entry.entry_id
        return pivot
    else:
        # Fallback to individual models if ensemble not available
        files = {
            "random_forest": predictions_dir
            / "random_forest"
            / entry.predictions_filename("random_forest"),
            "gradient_boosting": predictions_dir
            / "gradient_boosting"
            / entry.predictions_filename("gradient_boosting"),
        }

        dfs = []
        for model_name, file_path in files.items():
            if not file_path.exists():
                raise FileNotFoundError(f"Prediction file not found: {file_path}")
            df = pd.read_csv(file_path)
            df["model"] = model_name
            dfs.append(df)

        combined = pd.concat(dfs, ignore_index=True)
        pivot = (
            combined.pivot_table(
                index=["player_id", "player_name", "reference_date"],
                columns="model",
                values="injury_probability",
                aggfunc="first",
            )
            .reset_index()
            .sort_values("reference_date")
        )
        pivot.columns.name = None
        pivot["reference_date"] = pd.to_datetime(pivot["reference_date"])
        pivot["entry_id"] = entry.entry_id
        
        # Create ensemble from individual models if available
        if "random_forest" in pivot.columns and "gradient_boosting" in pivot.columns:
            pivot["ensemble"] = (pivot["random_forest"] + pivot["gradient_boosting"]) / 2.0
        
        return pivot


def save_combined_predictions(pivot: pd.DataFrame, output_path: Path) -> None:
    output_path.parent.mkdir(parents=True, exist_ok=True)
    pivot.to_csv(output_path, index=False, encoding="utf-8-sig")


def load_daily_feature_vector(entry: BacktestEntry, daily_dir: Path) -> pd.Series | None:
    src_file = daily_dir / entry.daily_features_filename
    if not src_file.exists():
        return None
    df = pd.read_csv(src_file)
    if "date" not in df.columns:
        return None
    df["date"] = pd.to_datetime(df["date"], errors="coerce")
    df.dropna(subset=["date"], inplace=True)
    target_row = df.loc[df["date"] == entry.window_end]
    if target_row.empty:
        return None
    row = target_row.iloc[0]
    return row.drop(labels=["date"])


def predict_insights(
    feature_row: pd.Series | None,
    bodypart_pipeline,
    severity_pipeline,
) -> Dict[str, object]:
    if feature_row is None:
        return {
            "bodypart_rank": [],
            "severity_probs": {},
            "severity_label": None,
        }
    X = feature_row.to_frame().T
    if hasattr(bodypart_pipeline, "feature_names_in_"):
        X = X.reindex(columns=bodypart_pipeline.feature_names_in_, fill_value=np.nan)

    body_probs = bodypart_pipeline.predict_proba(X)[0]
    severity_probs = severity_pipeline.predict_proba(X)[0]

    body_rank = rank_body_part_probabilities(body_probs, bodypart_pipeline.classes_)
    severity_dict = format_severity_probabilities(severity_probs, severity_pipeline.classes_)
    severity_label = severity_pipeline.classes_[int(np.argmax(severity_probs))]

    return {
        "bodypart_rank": body_rank,
        "severity_probs": severity_dict,
        "severity_label": severity_label,
    }


def load_shap_top_features(predictions_dir: Path, entry: BacktestEntry) -> List[Dict[str, object]]:
    shap_path = predictions_dir / "gradient_boosting" / "explanations" / f"{entry.entry_id}_shap.json"
    if not shap_path.exists():
        return []
    payload = json.loads(shap_path.read_text(encoding="utf-8"))
    return payload.get("top_features", [])


def select_final_top_features(shap_payload: List[Dict[str, object]], entry: BacktestEntry, limit: int = 10):
    target_date = entry.window_end.strftime("%Y-%m-%d")
    fallback = shap_payload[-1]["features"] if shap_payload else []
    for item in shap_payload:
        if item.get("reference_date") == target_date:
            features = item.get("features", [])
            return features[:limit]
    return fallback[:limit]


def format_feature_summary(features: List[Dict[str, object]], top_n: int = 5) -> str:
    lines = []
    for feature in features[:top_n]:
        value = feature.get("value", 0.0)
        sign = "+" if value >= 0 else "-"
        lines.append(f"{feature['name']} ({sign}{abs(value):.3f})")
    return ", ".join(lines)


def create_chart_for_entry(
    pivot: pd.DataFrame,
    entry: BacktestEntry,
    output_dir: Path,
    risk_df_ensemble: pd.DataFrame | None,
    insights: Dict | None = None,
    trend_metrics: Dict | None = None,
) -> Path:
    """Create Option 3 dashboard layout with smooth curves using ensemble model."""
    from scipy.interpolate import make_interp_spline
    import numpy as np
    
    player_name = pivot["player_name"].iloc[0]
    fig = plt.figure(figsize=(14, 8))
    gs = fig.add_gridspec(2, 3, height_ratios=[3, 2], hspace=0.35, wspace=0.45)

    # Top: Risk evolution chart (full width) - Smooth curve using ensemble
    ax_main = fig.add_subplot(gs[0, :])
    dates = pivot["reference_date"]
    
    if risk_df_ensemble is not None:
        risk_indices = risk_df_ensemble["risk_index"] + 1
        # Create smooth curve using interpolation
        dates_numeric = np.arange(len(dates))
        if len(dates) > 3:
            x_smooth = np.linspace(dates_numeric.min(), dates_numeric.max(), 500)
            spl = make_interp_spline(dates_numeric, risk_indices, k=min(3, len(dates)-1))
            y_smooth = spl(x_smooth)
            dates_smooth = pd.date_range(dates.min(), dates.max(), periods=500)
            
            # Map interpolated values back to risk classes for coloring
            colors_smooth = []
            for val in y_smooth:
                idx = int(np.clip(np.round(val - 1), 0, len(RISK_CLASS_LABELS) - 1))
                label = RISK_CLASS_LABELS[idx]
                colors_smooth.append(RISK_CLASS_COLORS[label])
            
            # Plot smooth line with gradient coloring using segments
            for i in range(len(dates_smooth) - 1):
                ax_main.plot(dates_smooth[i:i+2], y_smooth[i:i+2], 
                            color=colors_smooth[i], linewidth=3.5, alpha=0.85, zorder=3,
                            label="injury risk" if i == 0 else "")
            
            # Add area fill under the curve
            ax_main.fill_between(dates_smooth, y_smooth, alpha=0.15, color='gray', zorder=1)
        else:
            # For very few points, just plot a simple line
            colors = [RISK_CLASS_COLORS[label] for label in risk_df_ensemble["risk_label"]]
            for i in range(len(dates) - 1):
                ax_main.plot(dates[i:i+2], risk_indices[i:i+2], 
                            color=colors[i], linewidth=3.5, alpha=0.85, zorder=3,
                            label="injury risk" if i == 0 else "")
            ax_main.fill_between(dates, risk_indices, alpha=0.15, color='gray', zorder=1)

    injury_date = entry.injury_date
    has_injury_line = False
    if dates.min() <= injury_date <= dates.max():
        ax_main.axvline(injury_date, color="black", linestyle="--", linewidth=2, label="Injury Date", zorder=10)
        has_injury_line = True

    ax_main.set_title(f"Injury Risk Evolution - {player_name}", fontsize=14, fontweight="bold", pad=15)
    ax_main.set_ylabel("Risk Class", fontweight="bold")
    ax_main.set_yticks(range(1, len(RISK_CLASS_LABELS) + 1))
    ax_main.set_yticklabels(RISK_CLASS_LABELS)
    ax_main.set_ylim(0.5, len(RISK_CLASS_LABELS) + 0.5)
    if has_injury_line or risk_df_ensemble is not None:
        ax_main.legend(loc="upper left")
    ax_main.grid(axis="y", alpha=0.3, linestyle="--")
    plt.setp(ax_main.xaxis.get_majorticklabels(), rotation=45, ha="right", fontsize=7)

    # Bottom Left: Body Parts
    ax_body = fig.add_subplot(gs[1, 0])
    if insights and insights.get("bodypart_rank"):
        body_rank = insights.get("bodypart_rank", [])[:3]
        labels = [item[0].replace("_", " ").title() for item in body_rank]
        probs = [item[1] * 100 for item in body_rank]
        colors_body = ["#2E86AB", "#A23B72", "#F18F01"][:len(labels)]
        bars = ax_body.barh(labels, probs, color=colors_body, alpha=0.8, edgecolor="white", linewidth=1)
        ax_body.set_xlabel("Probability (%)", fontweight="bold")
        ax_body.set_title("Body Parts at Risk", fontweight="bold", pad=10)
        ax_body.set_xlim(0, 100)
        for i, (bar, prob) in enumerate(zip(bars, probs)):
            ax_body.text(prob + 2, i, f"{prob:.1f}%", va="center", fontweight="bold")
        ax_body.grid(axis="x", alpha=0.3, linestyle="--")
    else:
        ax_body.text(0.5, 0.5, "No data", ha="center", va="center", transform=ax_body.transAxes)
        ax_body.set_title("Body Parts at Risk", fontweight="bold")

    # Bottom Center: Severity
    ax_sev = fig.add_subplot(gs[1, 1])
    if insights and insights.get("severity_probs"):
        severity_probs = insights.get("severity_probs", {})
        severity_label = insights.get("severity_label", "Unknown")
        labels_sev = list(severity_probs.keys())
        sizes = [severity_probs[k] * 100 for k in labels_sev]
        colors_sev = ["#06A77D", "#F4A261", "#E76F51", "#264653"][:len(labels_sev)]
        bars_sev = ax_sev.barh(range(len(labels_sev)), sizes, color=colors_sev, alpha=0.8, edgecolor="white", linewidth=1)
        ax_sev.set_yticks(range(len(labels_sev)))
        # Replace underscores with spaces and format labels
        formatted_labels = []
        for l in labels_sev:
            formatted = l.replace("_", " ").title()
            # Special case: "Long Term" -> "Long term"
            if formatted == "Long Term":
                formatted = "Long term"
            formatted_labels.append(formatted)
        ax_sev.set_yticklabels(formatted_labels)
        ax_sev.set_xlabel("Probability (%)", fontweight="bold")
        formatted_severity_label = severity_label.replace("_", " ").title()
        if formatted_severity_label == "Long Term":
            formatted_severity_label = "Long term"
        ax_sev.set_title(f"Severity: {formatted_severity_label}", fontweight="bold", pad=10)
        ax_sev.set_xlim(0, 100)
        for i, (bar, size) in enumerate(zip(bars_sev, sizes)):
            ax_sev.text(size + 2, i, f"{size:.1f}%", va="center", fontweight="bold")
        ax_sev.grid(axis="x", alpha=0.3, linestyle="--")
    else:
        ax_sev.text(0.5, 0.5, "No data", ha="center", va="center", transform=ax_sev.transAxes)
        ax_sev.set_title("Severity", fontweight="bold")

    # Bottom Right: Alert Summary
    ax_alert = fig.add_subplot(gs[1, 2])
    ax_alert.axis("off")
    if risk_df_ensemble is not None and not risk_df_ensemble.empty:
        peak_risk = RISK_CLASS_LABELS[int(risk_df_ensemble["risk_index"].max())]
        final_risk = risk_df_ensemble["risk_label"].iloc[-1]
    else:
        peak_risk = "N/A"
        final_risk = "N/A"
    
    sustained = trend_metrics.get("sustained_elevated_days", 0) if trend_metrics else 0
    max_jump = trend_metrics.get("max_jump", 0.0) if trend_metrics else 0.0
    slope = trend_metrics.get("slope", 0.0) if trend_metrics else 0.0

    alert_text = f"""RISK SUMMARY

Peak Risk Level: {peak_risk}
Final Risk Level: {final_risk}
Sustained Elevated: {sustained} days
Max Daily Jump: {max_jump:.3f}
Trend Slope: {slope:.4f}"""
    ax_alert.text(0.1, 0.5, alert_text, transform=ax_alert.transAxes, fontsize=9, verticalalignment="center", family="monospace", fontweight="bold")

    plt.suptitle(
        f"Observation Period: {entry.window_start.strftime('%Y-%m-%d')} to {entry.window_end.strftime('%Y-%m-%d')}",
        fontsize=11,
        y=0.98,
    )

    output_dir.mkdir(parents=True, exist_ok=True)
    plot_path = output_dir / entry.chart_filename
    plt.savefig(plot_path, dpi=300, bbox_inches="tight", facecolor="white")
    plt.close(fig)
    return plot_path


def build_entry_summary(
    entry: BacktestEntry,
    pivot: pd.DataFrame,
    combined_path: Path,
    chart_path: Path,
    insights: Dict[str, object],
    shap_features: List[Dict[str, object]],
) -> Dict[str, object]:
    # Use ensemble predictions
    ensemble_series = pivot["ensemble"] if "ensemble" in pivot else pd.Series(dtype=float)
    risk_df = compute_risk_series(ensemble_series) if not ensemble_series.empty else None
    trend_metrics = compute_trend_metrics(ensemble_series) if not ensemble_series.empty else {}
    peak_risk = (
        RISK_CLASS_LABELS[int(risk_df["risk_index"].max())] if risk_df is not None else None
    )
    final_risk = (
        risk_df["risk_label"].iloc[-1] if risk_df is not None and not risk_df.empty else None
    )

    summary = {
        "entry_id": entry.entry_id,
        "player_id": entry.player_id,
        "player_name": pivot["player_name"].iloc[0],
        "injury_date": entry.injury_date.strftime("%Y-%m-%d"),
        "window_start": entry.window_start.strftime("%Y-%m-%d"),
        "window_end": entry.window_end.strftime("%Y-%m-%d"),
        "combined_predictions_file": str(combined_path),
        "chart_file": str(chart_path),
        "ensemble_peak_risk": peak_risk,
        "ensemble_final_risk": final_risk,
        "ensemble_max_prob": float(ensemble_series.max()) if not ensemble_series.empty else None,
        "ensemble_avg_prob": float(ensemble_series.mean()) if not ensemble_series.empty else None,
        "ensemble_final_prob": trend_metrics.get("final_probability"),
        "ensemble_max_jump": trend_metrics.get("max_jump"),
        "ensemble_slope": trend_metrics.get("slope"),
        "ensemble_sustained_elevated_days": trend_metrics.get("sustained_elevated_days"),
        "ensemble_final_deviation": trend_metrics.get("final_deviation"),
    }

    top_body_parts = insights.get("bodypart_rank", [])[:3]
    summary["top_body_parts"] = "; ".join(f"{label} ({prob:.2f})" for label, prob in top_body_parts)
    summary["severity_label"] = insights.get("severity_label")
    severity_dict = insights.get("severity_probs", {})
    summary["severity_distribution"] = "; ".join(
        f"{label}: {prob:.2f}" for label, prob in sorted(severity_dict.items(), key=lambda kv: kv[1], reverse=True)
    )
    summary["top_features"] = format_feature_summary(shap_features, top_n=5)

    return summary


def write_markdown(output_path: Path, summaries: List[Dict[str, object]]) -> None:
    with output_path.open("w", encoding="utf-8") as fp:
        fp.write("# Retrospective Backtesting Summary\n\n")
        for row in summaries:
            fp.write(f"## {row['player_name']} (ID: {row['player_id']}) â€” Entry {row['entry_id']}\n")
            fp.write(f"- Injury date: {row['injury_date']}\n")
            fp.write(
                f"- Observation window: {row['window_start']} to {row['window_end']} (45 days, injury day excluded)\n"
            )
            fp.write(f"- Gradient Boosting peak risk: {row.get('gb_peak_risk', 'N/A')}\n")
            fp.write(f"- Gradient Boosting final risk: {row.get('gb_final_risk', 'N/A')}\n")
            fp.write(f"- Gradient Boosting final probability: {format_prob(row.get('gb_final_prob'))}\n")
            fp.write(f"- Max daily jump (GB): {format_prob(row.get('gb_max_jump'))}\n")
            fp.write(
                f"- Sustained elevated days (GB): {row.get('gb_sustained_elevated_days', 'N/A')}\n"
            )
            fp.write(f"- Body-part risk focus: {row.get('top_body_parts', 'N/A')}\n")
            fp.write(f"- Severity profile: {row.get('severity_label', 'N/A')} ({row.get('severity_distribution', '')})\n")
            fp.write(f"- Top contributing features: {row.get('top_features', 'N/A')}\n")
            fp.write(f"- Combined predictions CSV: {row['combined_predictions_file']}\n")
            fp.write(f"- Chart: {row['chart_file']}\n\n")


def main() -> None:
    args = parse_args()
    bodypart_pipeline, _ = load_pipeline(Path("models") / "insights" / "bodypart_classifier")
    severity_pipeline, _ = load_pipeline(Path("models") / "insights" / "severity_classifier")

    if args.config:
        entries = load_backtest_config(args.config)
        combined_dir = args.output_dir / "combined"
        summary_rows: List[Dict[str, object]] = []

        print(f"Creating summaries and visuals using config: {args.config}")
        for entry in entries:
            try:
                pivot = load_predictions_for_entry(args.predictions_dir, entry)
            except Exception as exc:  # pylint: disable=broad-except
                print(f"[WARN] {entry.entry_id}: {exc}")
                continue

            combined_path = combined_dir / entry.combined_predictions_filename
            save_combined_predictions(pivot, combined_path)

            # Use ensemble predictions
            ensemble_risk_df = compute_risk_series(pivot["ensemble"]) if "ensemble" in pivot else None

            # Load insights before creating chart
            feature_row = load_daily_feature_vector(entry, args.daily_features_dir)
            insights = predict_insights(feature_row, bodypart_pipeline, severity_pipeline)
            ensemble_series = pivot["ensemble"] if "ensemble" in pivot else pd.Series(dtype=float)
            trend_metrics = compute_trend_metrics(ensemble_series) if not ensemble_series.empty else {}

            chart_path = create_chart_for_entry(
                pivot,
                entry,
                args.output_dir,
                ensemble_risk_df,
                insights,
                trend_metrics,
            )

            shap_payload = load_shap_top_features(args.predictions_dir, entry)
            top_features = select_final_top_features(shap_payload, entry, limit=10)

            summary = build_entry_summary(
                entry,
                pivot,
                combined_path,
                chart_path,
                insights,
                top_features,
            )
            summary_rows.append(summary)
            print(f"[OK] {entry.entry_id}: summary generated")

        if summary_rows:
            summary_df = pd.DataFrame(summary_rows)
            summary_csv = args.output_dir / "predictions_summary.csv"
            summary_df.to_csv(summary_csv, index=False, encoding="utf-8-sig")
            summary_md = args.output_dir / "predictions_summary.md"
            write_markdown(summary_md, summary_rows)
            print(f"[OK] Summary files written to {summary_csv} and {summary_md}")
        else:
            print("[WARN] No summaries generated")
        return

    windows = parse_windows(args.windows)
    combined_dir = args.output_dir / "combined"
    summary_rows: List[Dict[str, object]] = []

    print("Creating summaries and visuals...")
    for player_id, (start, end) in windows.items():
        try:
            entry = BacktestEntry(
                entry_id=f"player_{player_id}_{end.replace('-', '')}",
                player_id=player_id,
                injury_date=pd.to_datetime(end),
                season=None,
                injury_type=None,
                window_start=pd.to_datetime(start),
                window_end=pd.to_datetime(end),
                history_start=pd.to_datetime(start),
                history_end=pd.to_datetime(end),
            )
            pivot = load_predictions_for_entry(args.predictions_dir, entry)
        except Exception as exc:  # pylint: disable=broad-except
            print(f"[WARN] Player {player_id}: {exc}")
            continue

        combined_path = combined_dir / entry.combined_predictions_filename
        save_combined_predictions(pivot, combined_path)

        # Use ensemble predictions
        ensemble_risk_df = compute_risk_series(pivot["ensemble"]) if "ensemble" in pivot else None
        
        # Load insights before creating chart
        feature_row = load_daily_feature_vector(entry, args.daily_features_dir)
        insights = predict_insights(feature_row, bodypart_pipeline, severity_pipeline)
        ensemble_series = pivot["ensemble"] if "ensemble" in pivot else pd.Series(dtype=float)
        trend_metrics = compute_trend_metrics(ensemble_series) if not ensemble_series.empty else {}
        
        chart_path = create_chart_for_entry(
            pivot,
            entry,
            args.output_dir,
            ensemble_risk_df,
            insights,
            trend_metrics,
        )

        shap_payload = load_shap_top_features(args.predictions_dir, entry)
        top_features = select_final_top_features(shap_payload, entry, limit=10)

        summary = build_entry_summary(
            entry,
            pivot,
            combined_path,
            chart_path,
            insights,
            top_features,
        )
        summary_rows.append(summary)
        print(f"[OK] Player {player_id}: summary saved")

    if summary_rows:
        summary_df = pd.DataFrame(summary_rows)
        summary_csv = args.output_dir / "predictions_summary.csv"
        summary_df.to_csv(summary_csv, index=False, encoding="utf-8-sig")
        summary_md = args.output_dir / "predictions_summary.md"
        write_markdown(summary_md, summary_rows)
        print(f"[OK] Summary files written to {summary_csv} and {summary_md}")
    else:
        print("[WARN] No summaries generated")


if __name__ == "__main__":
    main()
