#!/usr/bin/env python3
"""Run retrospective predictions using optimized models.

This script loads the pre-generated timelines, applies both the optimized
Random Forest and Gradient Boosting models, and writes per-player daily
injury probabilities to `backtests/predictions/<model>/`.
"""
from __future__ import annotations

import argparse
import json
from pathlib import Path
from typing import Dict, Tuple

import joblib
import pandas as pd

import sys

ROOT_DIR = Path(__file__).resolve().parents[2]
if str(ROOT_DIR) not in sys.path:
    sys.path.append(str(ROOT_DIR))

from scripts.backtests.config_utils import BacktestEntry, load_backtest_config  # noqa: E402
from scripts.backtests.insight_utils import compute_shap_top_features  # noqa: E402

MODEL_CONFIG: Dict[str, Dict[str, Path]] = {
    "random_forest": {
        "model": Path("models") / "rf_model_combined_trainval.joblib",
        "columns": Path("models") / "rf_model_combined_trainval_columns.json",
    },
    "gradient_boosting": {
        "model": Path("models") / "gb_model_combined_trainval.joblib",
        "columns": Path("models") / "gb_model_combined_trainval_columns.json",
    },
}

DEFAULT_WINDOWS: Dict[int, Tuple[str, str]] = {
    452607: ("2025-01-01", "2025-02-08"),
    699592: ("2025-01-01", "2025-02-08"),
    258027: ("2025-09-01", "2025-10-29"),
    8198: ("2025-04-01", "2025-05-11"),
    200512: ("2024-04-01", "2024-05-31"),
}


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description=__doc__)
    parser.add_argument(
        "--config",
        type=Path,
        default=None,
        help="Optional JSON backtesting configuration generated by build_window_config.py.",
    )
    parser.add_argument(
        "--windows",
        type=str,
        default=None,
        help=(
            "Optional custom windows in the format "
            "player_id:start:end,player_id:start:end ... "
            "(dates in YYYY-MM-DD)."
        ),
    )
    parser.add_argument(
        "--timelines-dir",
        type=Path,
        default=Path("backtests") / "timelines",
        help="Directory containing the timeline CSVs.",
    )
    parser.add_argument(
        "--output-dir",
        type=Path,
        default=Path("backtests") / "predictions",
        help="Directory to write the prediction CSVs to.",
    )
    return parser.parse_args()


def parse_windows(raw: str | None) -> Dict[int, Tuple[str, str]]:
    if not raw:
        return DEFAULT_WINDOWS

    windows: Dict[int, Tuple[str, str]] = {}
    for item in raw.split(","):
        parts = item.strip().split(":")
        if len(parts) != 3:
            raise ValueError(f"Invalid window specification: {item!r}")
        player_id, start, end = parts
        windows[int(player_id)] = (start, end)
    return windows


def encode_features(df: pd.DataFrame, training_columns: list[str]) -> pd.DataFrame:
    features = df.copy()

    # Separate identifiers
    id_cols = ["player_id", "reference_date", "player_name"]
    meta = features[id_cols]
    features = features.drop(columns=id_cols)

    categorical_features = features.select_dtypes(include=["object"]).columns.tolist()
    numeric_features = features.select_dtypes(include=["int64", "float64"]).columns.tolist()

    for feature in categorical_features:
        features[feature] = features[feature].fillna("Unknown")
        dummies = pd.get_dummies(features[feature], prefix=feature, drop_first=True)
        features = pd.concat([features.drop(columns=[feature]), dummies], axis=1)

    for feature in numeric_features:
        if feature in features.columns:
            features[feature] = features[feature].fillna(0)

    # Align with training columns
    for col in training_columns:
        if col not in features.columns:
            features[col] = 0
    features = features[training_columns]

    # Drop extraneous columns (safety)
    missing_cols = set(features.columns) - set(training_columns)
    if missing_cols:
        features = features.drop(columns=list(missing_cols))

    # Restore identifiers for reference
    encoded = pd.concat([meta.reset_index(drop=True), features.reset_index(drop=True)], axis=1)
    return encoded


def load_model(model_path: Path, columns_path: Path):
    if not model_path.exists():
        raise FileNotFoundError(f"Model file not found: {model_path}")
    if not columns_path.exists():
        raise FileNotFoundError(f"Training column file not found: {columns_path}")

    model = joblib.load(model_path)
    training_columns = json.loads(columns_path.read_text(encoding="utf-8"))
    return model, training_columns


def run_predictions_for_player(
    player_id: int,
    start: str,
    end: str,
    timelines_dir: Path,
    output_dir: Path,
) -> None:
    input_file = timelines_dir / f"player_{player_id}_timelines_{start.replace('-', '')}_{end.replace('-', '')}.csv"
    if not input_file.exists():
        raise FileNotFoundError(f"Timeline file not found: {input_file}")

    df = pd.read_csv(input_file)
    if df.empty:
        raise ValueError(f"Timeline file {input_file} is empty")

    for model_name, paths in MODEL_CONFIG.items():
        model, training_columns = load_model(paths["model"], paths["columns"])
        encoded = encode_features(df, training_columns)

        meta = encoded[["player_id", "reference_date", "player_name"]]
        features = encoded.drop(columns=["player_id", "reference_date", "player_name"])

        probabilities = model.predict_proba(features)[:, 1]
        predictions = meta.copy()
        predictions["injury_probability"] = probabilities
        predictions["model"] = model_name

        model_output_dir = output_dir / model_name
        model_output_dir.mkdir(parents=True, exist_ok=True)
        output_file = model_output_dir / f"player_{player_id}_predictions_{start.replace('-', '')}_{end.replace('-', '')}.csv"
        predictions.to_csv(output_file, index=False, encoding="utf-8-sig")
        print(f"[OK] {model_name} -> {output_file}")


def run_predictions_for_entry(
    entry: BacktestEntry,
    timelines_dir: Path,
    output_dir: Path,
) -> None:
    input_file = timelines_dir / entry.timelines_filename
    if not input_file.exists():
        raise FileNotFoundError(f"Timeline file not found: {input_file}")

    df = pd.read_csv(input_file)
    if df.empty:
        raise ValueError(f"Timeline file {input_file} is empty")

    if "entry_id" not in df.columns:
        df["entry_id"] = entry.entry_id

    # Store probabilities from both models for ensemble
    rf_proba = None
    gb_proba = None
    meta = None

    for model_name, paths in MODEL_CONFIG.items():
        model, training_columns = load_model(paths["model"], paths["columns"])
        encoded = encode_features(df, training_columns)

        meta_cols = ["player_id", "reference_date", "player_name"]
        if "entry_id" in encoded.columns:
            meta_cols.append("entry_id")
        meta = encoded[meta_cols]
        features = encoded.drop(columns=meta_cols)

        probabilities = model.predict_proba(features)[:, 1]
        predictions = meta.copy()
        predictions["injury_probability"] = probabilities
        predictions["model"] = model_name
        predictions["injury_date"] = entry.injury_date.strftime("%Y-%m-%d")

        # Store probabilities for ensemble
        if model_name == "random_forest":
            rf_proba = probabilities
        elif model_name == "gradient_boosting":
            gb_proba = probabilities

        model_output_dir = output_dir / model_name
        model_output_dir.mkdir(parents=True, exist_ok=True)
        output_file = model_output_dir / entry.predictions_filename(model_name)
        predictions.to_csv(output_file, index=False, encoding="utf-8-sig")
        print(f"[OK] {model_name} -> {output_file}")

        if model_name == "gradient_boosting":
            shap_features = compute_shap_top_features(model, features)
            shap_payload = []
            for idx, row in enumerate(shap_features):
                shap_payload.append(
                    {
                        "reference_date": str(meta.iloc[idx]["reference_date"]),
                        "features": row,
                        "probability": float(probabilities[idx]),
                    }
                )
            explanations_dir = model_output_dir / "explanations"
            explanations_dir.mkdir(parents=True, exist_ok=True)
            shap_path = explanations_dir / f"{entry.entry_id}_shap.json"
            shap_path.write_text(
                json.dumps(
                    {
                        "entry_id": entry.entry_id,
                        "player_id": entry.player_id,
                        "model": model_name,
                        "top_features": shap_payload,
                    },
                    indent=2,
                ),
                encoding="utf-8",
            )

    # Generate ensemble predictions (average of RF and GB)
    if rf_proba is not None and gb_proba is not None and meta is not None:
        ensemble_proba = (rf_proba + gb_proba) / 2.0
        ensemble_predictions = meta.copy()
        ensemble_predictions["injury_probability"] = ensemble_proba
        ensemble_predictions["model"] = "ensemble"
        ensemble_predictions["injury_date"] = entry.injury_date.strftime("%Y-%m-%d")

        ensemble_output_dir = output_dir / "ensemble"
        ensemble_output_dir.mkdir(parents=True, exist_ok=True)
        ensemble_output_file = ensemble_output_dir / entry.predictions_filename("ensemble")
        ensemble_predictions.to_csv(ensemble_output_file, index=False, encoding="utf-8-sig")
        print(f"[OK] ensemble -> {ensemble_output_file}")


def main() -> None:
    args = parse_args()
    if args.config:
        entries = load_backtest_config(args.config)
        print(f"Running retrospective predictions using config: {args.config}")
        for entry in entries:
            try:
                run_predictions_for_entry(
                    entry=entry,
                    timelines_dir=args.timelines_dir,
                    output_dir=args.output_dir,
                )
            except Exception as exc:  # pylint: disable=broad-except
                print(f"[WARN] {entry.entry_id}: {exc}")
        return

    windows = parse_windows(args.windows)

    print("Running retrospective predictions...")
    for player_id, (start, end) in windows.items():
        try:
            run_predictions_for_player(
                player_id,
                start,
                end,
                timelines_dir=args.timelines_dir,
                output_dir=args.output_dir,
            )
        except Exception as exc:  # pylint: disable=broad-except
            print(f"[WARN] Player {player_id}: {exc}")


if __name__ == "__main__":
    main()
